# Comprehensive Alert Rules for Alertmanager
# ===========================================
#
# This file contains comprehensive alerting rules for:
# - System Performance (CPU, Memory, Disk, Load)
# - System Health (Network, Processes, Time)
# - Docker Containers (Health, Resources)
# - Monitoring Stack (Prometheus, Alertmanager, Grafana)
# - Network Services (Endpoints, SSL, Latency)
#
# Alert Levels:
# - WARNING: Requires attention, potential issues
# - CRITICAL: Requires immediate action, service impacting
#
# Routing:
# - WARNING alerts → Email notifications
# - CRITICAL alerts → Email + Telegram notifications
#
# Configuration:
# - Telegram alerts require bot token and chat ID in inventory
# - Email alerts require SMTP configuration in defaults
#
# Usage:
# Deploy with: ansible-playbook orchestrate.yml -e "roles_enabled=['alertmanager']"
#
groups:
  - name: system_performance_alerts
    rules:
      # CPU Monitoring
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{job="node", environment="prod", mode="idle"}[5m])) * 100) > 80
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: system
          component: cpu
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage on {{ $labels.instance }} is above 80% for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}%."
      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{job="node", environment="prod", mode="idle"}[5m])) * 100) > 95
        for: 30s  # Reduced from 1m for faster response
        labels:
          severity: critical
          service: system
          component: cpu
        annotations:
          summary: "Critical CPU usage on {{ $labels.instance }}"
          description: "CPU usage on {{ $labels.instance }} is above 95% for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}%. Risk: system unresponsive."
      # Memory Monitoring
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes{job="node", environment="prod"} - node_memory_MemAvailable_bytes{job="node", environment="prod"}) / node_memory_MemTotal_bytes{job="node", environment="prod"} * 100 > 85 and node_memory_MemTotal_bytes{job="node", environment="prod"} > 0
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: system
          component: memory
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage on {{ $labels.instance }} is above 85% for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}%."
      - alert: CriticalMemoryUsage
        expr: (node_memory_MemTotal_bytes{job="node", environment="prod"} - node_memory_MemAvailable_bytes{job="node", environment="prod"}) / node_memory_MemTotal_bytes{job="node", environment="prod"} * 100 > 95 and node_memory_MemTotal_bytes{job="node", environment="prod"} > 0
        for: 30s  # Reduced from 1m for faster response
        labels:
          severity: critical
          service: system
          component: memory
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: "Memory usage on {{ $labels.instance }} is above 95% for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}%. Risk: heavy swapping."
      - alert: LowAvailableMemory
        expr: node_memory_MemAvailable_bytes{job="node", environment="prod"} / 1024 / 1024 / 1024 < 0.5
        for: 30s  # Reduced from 1m for faster response
        labels:
          severity: critical
          service: system
          component: memory
        annotations:
          summary: "Very low available memory on {{ $labels.instance }}"
          description: "Available memory on {{ $labels.instance }} is below 500MB for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}GB free."
      # Disk Space Monitoring
      - alert: HighDiskUsage
        expr: (node_filesystem_size_bytes{job="node", environment="prod", mountpoint!~"/snap/.*|/run/.*|/sys/.*|/proc/.*|/dev/.*|/boot/.*"} - node_filesystem_avail_bytes{job="node", environment="prod", mountpoint!~"/snap/.*|/run/.*|/sys/.*|/proc/.*|/dev/.*|/boot/.*"}) / node_filesystem_size_bytes{job="node", environment="prod", mountpoint!~"/snap/.*|/run/.*|/sys/.*|/proc/.*|/dev/.*|/boot/.*"} * 100 > 85 and node_filesystem_size_bytes{job="node", environment="prod", mountpoint!~"/snap/.*|/run/.*|/sys/.*|/proc/.*|/dev/.*|/boot/.*"} > 0
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: system
          component: disk
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage on {{ $labels.instance }} ({{ $labels.mountpoint }}) is above 85% for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}%."
      - alert: CriticalDiskUsage
        expr: (node_filesystem_size_bytes{job="node", environment="prod", mountpoint!~"/snap/.*|/run/.*|/sys/.*|/proc/.*|/dev/.*|/boot/.*"} - node_filesystem_avail_bytes{job="node", environment="prod", mountpoint!~"/snap/.*|/run/.*|/sys/.*|/proc/.*|/dev/.*|/boot/.*"}) / node_filesystem_size_bytes{job="node", environment="prod", mountpoint!~"/snap/.*|/run/.*|/sys/.*|/proc/.*|/dev/.*|/boot/.*"} * 100 > 95 and node_filesystem_size_bytes{job="node", environment="prod", mountpoint!~"/snap/.*|/run/.*|/sys/.*|/proc/.*|/dev/.*|/boot/.*"} > 0
        for: 30s  # Reduced from 1m for faster response
        labels:
          severity: critical
          service: system
          component: disk
        annotations:
          summary: "Critical disk usage on {{ $labels.instance }}"
          description: "Disk usage on {{ $labels.instance }} ({{ $labels.mountpoint }}) is above 95% for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}%. Filesystem may become read-only."
      - alert: LowDiskSpace
        expr: node_filesystem_avail_bytes{job="node", environment="prod", mountpoint!~"/snap/.*|/run/.*|/sys/.*|/proc/.*|/dev/.*|/boot/.*"} / 1024 / 1024 / 1024 < 2
        for: 30s  # Reduced from 1m for faster response
        labels:
          severity: critical
          service: system
          component: disk
        annotations:
          summary: "Very low disk space on {{ $labels.instance }}"
          description: "Available disk space on {{ $labels.instance }} ({{ $labels.mountpoint }}) is below 2GB for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}GB free."

  - name: system_health_alerts
    rules:
      # System Load
      - alert: HighSystemLoad
        expr: node_load5{job="node", environment="prod"} / count without(cpu, mode) (node_cpu_seconds_total{job="node", environment="prod", mode="idle"}) > 3
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: system
          component: load
        annotations:
          summary: "High system load on {{ $labels.instance }}"
          description: "System load on {{ $labels.instance }} is above 3x CPU cores for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}."
      - alert: CriticalSystemLoad
        expr: node_load5{job="node", environment="prod"} / count without(cpu, mode) (node_cpu_seconds_total{job="node", environment="prod", mode="idle"}) > 5
        for: 30s  # Reduced from 1m for faster response
        labels:
          severity: critical
          service: system
          component: load
        annotations:
          summary: "Critical system load on {{ $labels.instance }}"
          description: "System load on {{ $labels.instance }} is above 5x CPU cores for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}. System may become unresponsive."
      # Network Issues
      - alert: NetworkInterfaceDown
        expr: node_network_up{job="node", environment="prod", device!~"lo|docker.*|veth.*"} == 0 and up{job="node", environment="prod"} == 1
        for: 30s  # Reduced from 2m for faster response
        labels:
          severity: critical
          service: system
          component: network
        annotations:
          summary: "Network interface {{ $labels.device }} is down on {{ $labels.instance }}"
          description: "Network interface {{ $labels.device }} on {{ $labels.instance }} has been down for more than 30 seconds. Check connectivity and interface status."
      - alert: HighNetworkErrors
        expr: rate(node_network_receive_errs_total{job="node", environment="prod", device!~"lo|docker.*|veth.*"}[5m]) > 10
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: system
          component: network
        annotations:
          summary: "High network receive errors on {{ $labels.instance }}"
          description: "Network interface {{ $labels.device }} on {{ $labels.instance }} has a receive error rate of {{ $value | printf \"%.2f\" }} errors/sec for more than 2 minutes."
      # System Processes
      - alert: ZombieProcesses
        expr: node_processes_state{job="node", environment="prod", state="Z"} > 5
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: system
          component: processes
        annotations:
          summary: "High number of zombie processes on {{ $labels.instance }}"
          description: "There are {{ $value | printf \"%.0f\" }} zombie processes on {{ $labels.instance }} for more than 2 minutes."
      # System Time Drift
      - alert: SystemClockDrift
        expr: abs(node_timex_offset_seconds{job="node", environment="prod"}) > 0.5
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: system
          component: time
        annotations:
          summary: "System clock drift detected on {{ $labels.instance }}"
          description: "System clock offset on {{ $labels.instance }} is {{ $value | printf \"%.3f\" }} seconds for more than 2 minutes. Check NTP configuration."

  - name: docker_container_alerts
    rules:
      # Container Health
      - alert: ContainerDown
        expr: absent(container_last_seen{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+", status="running"}) or min_over_time(container_last_seen{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+", status="running"}[5m]) == 0
        for: 30s  # Reduced from 2m for faster response
        labels:
          severity: critical
          service: docker
          component: container
        annotations:
          summary: "Container {{ $labels.name }} is down on {{ $labels.instance }}"
          description: "Container {{ $labels.name }} in namespace {{ $labels.namespace }} on {{ $labels.instance }} has not been seen in running state for more than 30 seconds. It may have crashed or stopped."
      - alert: ContainerRestartTooOften
        expr: rate(kube_pod_container_status_restarts_total{job="kube-state-metrics", environment="prod", namespace=~"prod-.*", container=~".+"}[5m]) > 0.1
        for: 2m  # Reduced from 2m for consistency
        labels:
          severity: warning
          service: docker
          component: container
        annotations:
          summary: "Container {{ $labels.container }} restarting frequently"
          description: "Container {{ $labels.container }} in namespace {{ $labels.namespace }} on {{ $labels.instance }} is restarting at {{ $value | printf \"%.2f\" }} restarts/sec for more than 2 minutes. Check container logs."
      # Container Resource Usage
      - alert: HighContainerMemoryUsage
        expr: (container_memory_usage_bytes{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+"} / container_spec_memory_limit_bytes{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+"}) * 100 > 80 and container_spec_memory_limit_bytes{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+"} > 0
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: docker
          component: memory
        annotations:
          summary: "High memory usage in container {{ $labels.name }}"
          description: "Container {{ $labels.name }} in namespace {{ $labels.namespace }} on {{ $labels.instance }} is using {{ $value | printf \"%.2f\" }}% of memory limit for more than 2 minutes."
      - alert: CriticalContainerMemoryUsage
        expr: (container_memory_usage_bytes{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+"} / container_spec_memory_limit_bytes{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+"}) * 100 > 95 and container_spec_memory_limit_bytes{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+"} > 0
        for: 30s  # Reduced from 1m for faster response
        labels:
          severity: critical
          service: docker
          component: memory
        annotations:
          summary: "Critical memory usage in container {{ $labels.name }}"
          description: "Container {{ $labels.name }} in namespace {{ $labels.namespace }} on {{ $labels.instance }} is using {{ $value | printf \"%.2f\" }}% of memory limit for more than 30 seconds. Risk: OOM kill."
      - alert: HighContainerCPUUsage
        expr: rate(container_cpu_usage_seconds_total{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+"}[5m]) / container_spec_cpu_quota{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+"} * 100 > 80 and container_spec_cpu_quota{job="cadvisor", environment="prod", namespace=~"prod-.*", name=~".+"} > 0
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: docker
          component: cpu
        annotations:
          summary: "High CPU usage in container {{ $labels.name }}"
          description: "Container {{ $labels.name }} in namespace {{ $labels.namespace }} on {{ $labels.instance }} is using {{ $value | printf \"%.2f\" }}% of CPU quota for more than 2 minutes."
      # Docker Engine Health
      - alert: DockerDaemonDown
        expr: node_systemd_unit_state{job="node", environment="prod", name="docker.service", state="active"} == 0 and up{job="node", environment="prod"} == 1
        for: 30s  # Reduced from 2m for faster response
        labels:
          severity: critical
          service: docker
          component: daemon
        annotations:
          summary: "Docker daemon is down on {{ $labels.instance }}"
          description: "Docker daemon on {{ $labels.instance }} is not running for more than 30 seconds. Check service status and logs."

  - name: monitoring_alerts
    rules:
      # Prometheus Health
      - alert: PrometheusDown
        expr: up{job="prometheus", environment="prod"} == 0
        for: 30s  # Reduced from 2m for faster response
        labels:
          severity: critical
          service: monitoring
          component: prometheus
        annotations:
          summary: "Prometheus is down on {{ $labels.instance }}"
          description: "Prometheus instance {{ $labels.instance }} has been down for more than 30 seconds. No metrics collection is happening."
      - alert: PrometheusConfigurationReloadFailure
        expr: prometheus_config_last_reload_successful{job="prometheus", environment="prod"} == 0 and up{job="prometheus", environment="prod"} == 1
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: monitoring
          component: prometheus
        annotations:
          summary: "Prometheus configuration reload failed on {{ $labels.instance }}"
          description: "Prometheus instance {{ $labels.instance }} failed to reload configuration for more than 2 minutes. Check configuration file."
      # Alertmanager Health
      - alert: AlertmanagerDown
        expr: up{job="alertmanager", environment="prod"} == 0
        for: 30s  # Reduced from 2m for faster response
        labels:
          severity: critical
          service: monitoring
          component: alertmanager
        annotations:
          summary: "Alertmanager is down on {{ $labels.instance }}"
          description: "Alertmanager instance {{ $labels.instance }} has been down for more than 30 seconds. No alerts will be processed."
      # Node Exporter Health
      - alert: NodeExporterDown
        expr: up{job="node", environment="prod"} == 0
        for: 30s  # Reduced from 1m for faster response
        labels:
          severity: warning
          service: monitoring
          component: node-exporter
        annotations:
          summary: "Node Exporter is down on {{ $labels.instance }}"
          description: "Node Exporter on {{ $labels.instance }} has been down for more than 30 seconds. System metrics are not being collected."
      # Grafana Health
      - alert: GrafanaDown
        expr: up{job="grafana", environment="prod"} == 0
        for: 30s  # Reduced from 1m for faster response
        labels:
          severity: warning
          service: monitoring
          component: grafana
        annotations:
          summary: "Grafana is down on {{ $labels.instance }}"
          description: "Grafana on {{ $labels.instance }} has been down for more than 30 seconds. Dashboard access is unavailable."

  - name: network_service_alerts
    rules:
      # Blackbox Monitoring
      - alert: EndpointDown
        expr: min_over_time(probe_success{job=~".*blackbox.*", environment="prod", instance=~".+"}[30s]) == 0 and up{job=~".*blackbox.*", environment="prod"} == 1
        for: 15s  # Reduced from 2m for faster response, as per your request for probe_success
        labels:
          severity: critical
          service: network
          component: endpoint
        annotations:
          summary: "Endpoint {{ $labels.instance }} is down"
          description: "Endpoint {{ $labels.instance }} (module {{ $labels.module }}) has been unreachable for more than 15 seconds. Check endpoint availability."
      - alert: HighResponseTime
        expr: probe_duration_seconds{job=~".*blackbox.*", environment="prod", instance=~".+"} > 5
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: network
          component: latency
        annotations:
          summary: "High response time for {{ $labels.instance }}"
          description: "Response time for {{ $labels.instance }} (module {{ $labels.module }}) is above 5 seconds for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}s."
      - alert: SSLExpiryWarning
        expr: (probe_ssl_earliest_cert_expiry{job=~".*blackbox.*", environment="prod", instance=~".+"} - time()) / 86400 < 30
        for: 2m  # Reduced from 5m for faster response
        labels:
          severity: warning
          service: network
          component: ssl
        annotations:
          summary: "SSL certificate expiring soon for {{ $labels.instance }}"
          description: "SSL certificate for {{ $labels.instance }} (module {{ $labels.module }}) will expire in {{ $value | printf \"%.0f\" }} days. Check certificate renewal."
      - alert: SSLExpiryCritical
        expr: (probe_ssl_earliest_cert_expiry{job=~".*blackbox.*", environment="prod", instance=~".+"} - time()) / 86400 < 7
        for: 30s  # Reduced from 2m for faster response
        labels:
          severity: critical
          service: network
          component: ssl
        annotations:
          summary: "SSL certificate expiring very soon for {{ $labels.instance }}"
          description: "SSL certificate for {{ $labels.instance }} (module {{ $labels.module }}) will expire in {{ $value | printf \"%.0f\" }} days. Immediate renewal required."