# Comprehensive Alert Rules for Prometheus
# ==========================================
#
# This file contains comprehensive alerting rules for:
# - System Performance (CPU, Memory, Disk, Load)
# - System Health (Network, Processes, Time)
# - Docker Containers (Health, Resources)
# - Monitoring Stack (Prometheus, Alertmanager, Grafana)
# - Network Services (Endpoints, SSL, Latency)
#
# Alert Levels:
# - WARNING: Requires attention, potential issues
# - CRITICAL: Requires immediate action, service impacting
#
# Routing:
# - WARNING alerts → Email notifications
# - CRITICAL alerts → Email + Telegram notifications
#
# Configuration:
# - Telegram alerts require bot token and chat ID in inventory
# - Email alerts require SMTP configuration in defaults
#
# Usage:
# Deploy with: ansible-playbook orchestrate.yml -e "roles_enabled=['prometheus']"
#
groups:
  - name: system_performance_alerts
    rules:
      # CPU Monitoring
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{job= "node_exporter", mode="idle"}[5m])) * 100) > 80
        for: 2m
        labels:
          severity: warning
          service: system
          component: cpu
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage on {{ $labels.instance }} is above 80% for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}%."
      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{job= "node_exporter", mode="idle"}[5m])) * 100) > 95
        for: 30s
        labels:
          severity: critical
          service: system
          component: cpu
        annotations:
          summary: "Critical CPU usage on {{ $labels.instance }}"
          description: "CPU usage on {{ $labels.instance }} is above 95% for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}%. Risk: system unresponsive."
      # Memory Monitoring
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes{job="node_exporter" } - node_memory_MemAvailable_bytes{job="node_exporter" }) / node_memory_MemTotal_bytes{job="node_exporter" } * 100 > 85 and node_memory_MemTotal_bytes{job="node_exporter" } > 0
        for: 2m
        labels:
          severity: warning
          service: system
          component: memory
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage on {{ $labels.instance }} is above 85% for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}%."
      - alert: CriticalMemoryUsage
        expr: (node_memory_MemTotal_bytes{job="node_exporter" } - node_memory_MemAvailable_bytes{job="node_exporter" }) / node_memory_MemTotal_bytes{job="node_exporter" } * 100 > 95 and node_memory_MemTotal_bytes{job="node_exporter" } > 0
        for: 30s
        labels:
          severity: critical
          service: system
          component: memory
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: "Memory usage on {{ $labels.instance }} is above 95% for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}%. Risk: heavy swapping."
      - alert: LowAvailableMemory
        expr: node_memory_MemAvailable_bytes{job="node_exporter" } / 1024 / 1024 / 1024 < 0.5
        for: 30s
        labels:
          severity: critical
          service: system
          component: memory
        annotations:
          summary: "Very low available memory on {{ $labels.instance }}"
          description: "Available memory on {{ $labels.instance }} is below 500MB for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}GB free."
      # Disk Space Monitoring
      - alert: HighDiskUsage
        expr: (node_filesystem_size_bytes{job="node_exporter", mountpoint!~"/snap.*|/run.*|/sys.*|/proc.*|/dev.*|/boot.*"} - node_filesystem_avail_bytes{job="node_exporter", mountpoint!~"/snap.*|/run.*|/sys.*|/proc.*|/dev.*|/boot.*"}) / node_filesystem_size_bytes{job="node_exporter", mountpoint!~"/snap.*|/run.*|/sys.*|/proc.*|/dev.*|/boot.*"} * 100 > 85 and node_filesystem_size_bytes{job="node_exporter", mountpoint!~"/snap.*|/run.*|/sys.*|/proc.*|/dev.*|/boot.*"} > 0
        for: 2m
        labels:
          severity: warning
          service: system
          component: disk
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage on {{ $labels.instance }} ({{ $labels.mountpoint }}) is above 85% for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}%."
      - alert: CriticalDiskUsage
        expr: (node_filesystem_size_bytes{job="node_exporter", mountpoint!~"/snap.*|/run.*|/sys.*|/proc.*|/dev.*|/boot.*"} - node_filesystem_avail_bytes{job="node_exporter", mountpoint!~"/snap.*|/run.*|/sys.*|/proc.*|/dev.*|/boot.*"}) / node_filesystem_size_bytes{job="node_exporter", mountpoint!~"/snap.*|/run.*|/sys.*|/proc.*|/dev.*|/boot.*"} * 100 > 95 and node_filesystem_size_bytes{job="node_exporter", mountpoint!~"/snap.*|/run.*|/sys.*|/proc.*|/dev.*|/boot.*"} > 0
        for: 30s
        labels:
          severity: critical
          service: system
          component: disk
        annotations:
          summary: "Critical disk usage on {{ $labels.instance }}"
          description: "Disk usage on {{ $labels.instance }} ({{ $labels.mountpoint }}) is above 95% for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}%. Filesystem may become read-only."
      - alert: LowDiskSpace
        expr: node_filesystem_avail_bytes{job="node_exporter", mountpoint!~"/snap.*|/run.*|/sys.*|/proc.*|/dev.*|/boot.*"} / 1024 / 1024 / 1024 < 2
        for: 30s
        labels:
          severity: critical
          service: system
          component: disk
        annotations:
          summary: "Very low disk space on {{ $labels.instance }}"
          description: "Available disk space on {{ $labels.instance }} ({{ $labels.mountpoint }}) is below 2GB for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}GB free."

  - name: system_health_alerts
    rules:
      # System Load
      - alert: HighSystemLoad
        expr: node_load5{job="node_exporter" } / count without(cpu, mode) (node_cpu_seconds_total{job="node_exporter", mode="idle"}) > 3
        for: 2m
        labels:
          severity: warning
          service: system
          component: load
        annotations:
          summary: "High system load on {{ $labels.instance }}"
          description: "System load on {{ $labels.instance }} is above 3x CPU cores for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}."
      - alert: CriticalSystemLoad
        expr: node_load5{job="node_exporter" } / count without(cpu, mode) (node_cpu_seconds_total{job="node_exporter", mode="idle"}) > 5
        for: 30s
        labels:
          severity: critical
          service: system
          component: load
        annotations:
          summary: "Critical system load on {{ $labels.instance }}"
          description: "System load on {{ $labels.instance }} is above 5x CPU cores for more than 30 seconds. Current: {{ $value | printf \"%.2f\" }}. System may become unresponsive."
      # Network Issues
      - alert: NetworkInterfaceDown
        expr: node_network_up{job="node_exporter", device!~"lo|docker.*|veth.*"} == 0 and up{job="node_exporter" } == 1
        for: 30s
        labels:
          severity: critical
          service: system
          component: network
        annotations:
          summary: "Network interface {{ $labels.device }} is down on {{ $labels.instance }}"
          description: "Network interface {{ $labels.device }} on {{ $labels.instance }} has been down for more than 30 seconds. Check connectivity and interface status."
      - alert: HighNetworkErrors
        expr: rate(node_network_receive_errs_total{job="node_exporter", device!~"lo|docker.*|veth.*"}[5m]) > 10
        for: 2m
        labels:
          severity: warning
          service: system
          component: network
        annotations:
          summary: "High network receive errors on {{ $labels.instance }}"
          description: "Network interface {{ $labels.device }} on {{ $labels.instance }} has a receive error rate of {{ $value | printf \"%.2f\" }} errors/sec for more than 2 minutes."
      # System Processes
      - alert: ZombieProcesses
        expr: node_processes_state{job="node_exporter", state="Z"} > 5
        for: 2m
        labels:
          severity: warning
          service: system
          component: processes
        annotations:
          summary: "High number of zombie processes on {{ $labels.instance }}"
          description: "There are {{ $value | printf \"%.0f\" }} zombie processes on {{ $labels.instance }} for more than 2 minutes."
      # System Time Drift
      - alert: SystemClockDrift
        expr: abs(node_timex_offset_seconds{job="node_exporter" }) > 0.5
        for: 2m
        labels:
          severity: warning
          service: system
          component: time
        annotations:
          summary: "System clock drift detected on {{ $labels.instance }}"
          description: "System clock offset on {{ $labels.instance }} is {{ $value | printf \"%.3f\" }} seconds for more than 2 minutes. Check NTP configuration."

  - name: container_resource_alerts
    rules:
      # Container CPU Alerts
      - alert: ContainerHighCPUUsage
        expr: rate(container_cpu_usage_seconds_total{job="cadvisor", id!="/"}[5m]) * 100 > 80
        for: 2m  
        labels:
          severity: warning
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} high CPU usage"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has CPU usage at {{ $value | printf \"%.2f\" }}% for more than 2 minutes."
      - alert: ContainerCriticalCPUUsage
        expr: rate(container_cpu_usage_seconds_total{job="cadvisor", id!="/"}[5m]) * 100 > 95
        for: 1m  
        labels:
          severity: critical
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} critical CPU usage"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has CPU usage at {{ $value | printf \"%.2f\" }}% for more than 1 minute. Immediate action required."
      # Container Memory Alerts (absolute values for containers without limits)
      - alert: ContainerHighMemoryUsage
        expr: container_memory_usage_bytes{job="cadvisor", id!="/"} > 2 * 1024 * 1024 * 1024
        for: 5m  
        labels:
          severity: warning
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} high memory usage"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} is using {{ $value | humanize1024 }}B of memory for more than 5 minutes."
      - alert: ContainerVeryHighMemoryUsage
        expr: container_memory_usage_bytes{job="cadvisor", id!="/"} > 4 * 1024 * 1024 * 1024
        for: 2m  
        labels:
          severity: critical
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} very high memory usage"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} is using {{ $value | humanize1024 }}B of memory for more than 2 minutes. May impact system performance."
      # Container Memory Limit Alerts (only for containers with limits)
      - alert: ContainerMemoryLimitReached
        expr: container_memory_usage_bytes{job="cadvisor", id!="/"} >= container_spec_memory_limit_bytes{job="cadvisor", id!="/"} and container_spec_memory_limit_bytes{job="cadvisor", id!="/"} > 0
        for: 15s  
        labels:
          severity: critical
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} memory limit reached"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has reached its memory limit and may be killed by OOMKiller."
      # Container Disk I/O Alerts
      - alert: ContainerHighDiskIO
        expr: (rate(container_fs_reads_bytes_total{job="cadvisor", id!="/"}[5m]) + rate(container_fs_writes_bytes_total{job="cadvisor", id!="/"}[5m])) > 100 * 1024 * 1024
        for: 2m  
        labels:
          severity: warning
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} high disk I/O"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has high disk I/O at {{ $value | printf \"%.0f\" }} bytes/s for more than 2 minutes."
      # Container Network Alerts
      - alert: ContainerHighNetworkIO
        expr: (rate(container_network_receive_bytes_total{job="cadvisor", id!="/"}[5m]) + rate(container_network_transmit_bytes_total{job="cadvisor", id!="/"}[5m])) > 100 * 1024 * 1024
        for: 2m  
        labels:
          severity: warning
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} high network I/O"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has high network I/O at {{ $value | printf \"%.0f\" }} bytes/s for more than 2 minutes."

  - name: container_availability_alerts
    rules:
      # Container Health (using container memory usage as a proxy for running containers)
      - alert: ContainerDown
        expr: absent_over_time(container_memory_usage_bytes{job="cadvisor", id!="/"}[5m])
        for: 30s
        labels:
          severity: critical
          service: docker
          component: container
        annotations:
          summary: "Container {{ $labels.name }} is down on {{ $labels.instance }}"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has not been seen for more than 5 minutes. It may have crashed or stopped."
      # Container Restart Alerts
      - alert: ContainerRestartTooOften
        expr: rate(container_status_restarts_total{job="cadvisor", id!="/"}[5m]) > 0
        for: 2m  
        labels:
          severity: warning
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} restarting frequently"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} is restarting at {{ $value | printf \"%.2f\" }} restarts/sec. Check container logs for issues."
      # Container State Alerts

  - name: container_performance_alerts
    rules:
      # Container Throttling Alerts
      - alert: ContainerCPUThrottling
        expr: rate(container_cpu_cfs_throttled_seconds_total{job="cadvisor", id!="/"}[5m]) > 0.01
        for: 2m  
        labels:
          severity: warning
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} CPU throttling"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} is experiencing CPU throttling at {{ $value | printf \"%.2f\" }}s/s for more than 2 minutes."
      # Container File Descriptor Alerts (absolute values)
      - alert: ContainerHighFileDescriptorUsage
        expr: container_file_descriptors{job="cadvisor"} > 1000
        for: 2m  
        labels:
          severity: warning
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} high file descriptor usage"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has {{ $value | printf \"%.0f\" }} file descriptors open for more than 2 minutes."
      # Container Socket Usage Alerts
      - alert: ContainerHighSocketUsage
        expr: container_sockets{job="cadvisor"} > 1000
        for: 2m  
        labels:
          severity: warning
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} high socket usage"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has {{ $value | printf \"%.0f\" }} open sockets for more than 2 minutes."

  - name: container_security_alerts
    rules:
      # Container Security Alerts
      - alert: ContainerRunningAsRoot
        expr: container_processes{job="cadvisor", id!="/", user="root"} > 0
        for: 15s  
        labels:
          severity: warning
          service: container-monitoring
        annotations:
          summary: "Container {{ $labels.name }} running as root"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has {{ $value | printf \"%.0f\" }} processes running as root user."

  - name: monitoring_alerts
    rules:
      # Prometheus Health
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 30s
        labels:
          severity: critical
          service: monitoring
          component: prometheus
        annotations:
          summary: "Prometheus is down on {{ $labels.instance }}"
          description: "Prometheus instance {{ $labels.instance }} has been down for more than 30 seconds. No metrics collection is happening."
      - alert: PrometheusConfigurationReloadFailure
        expr: prometheus_config_last_reload_successful{job="prometheus"} == 0 and up{job="prometheus"} == 1
        for: 2m
        labels:
          severity: warning
          service: monitoring
          component: prometheus
        annotations:
          summary: "Prometheus configuration reload failed on {{ $labels.instance }}"
          description: "Prometheus instance {{ $labels.instance }} failed to reload configuration for more than 2 minutes. Check configuration file."
      # Node Exporter Health
      - alert: NodeExporterDown
        expr: up{job="node_exporter"} == 0
        for: 30s
        labels:
          severity: warning
          service: monitoring
          component: node-exporter
        annotations:
          summary: "Node Exporter is down on {{ $labels.instance }}"
          description: "Node Exporter on {{ $labels.instance }} has been down for more than 30 seconds. System metrics are not being collected."
      # Grafana Health (Container-based monitoring)
      - alert: GrafanaContainerDown
        expr: absent(container_memory_usage_bytes{name="grafana"}) and up{job="cadvisor"} == 1
        for: 30s
        labels:
          severity: warning
          service: monitoring
          component: grafana
        annotations:
          summary: "Grafana container is down on {{ $labels.instance }}"
          description: "Grafana container on {{ $labels.instance }} has been down for more than 30 seconds. Dashboard access is unavailable."
      # Docker Engine Health
      - alert: DockerDaemonDown
        expr: node_systemd_unit_state{job="node_exporter", name="docker.service", state="active"} == 0 and up{job="node_exporter"} == 1
        for: 30s
        labels:
          severity: critical
          service: docker
          component: daemon
        annotations:
          summary: "Docker daemon is down on {{ $labels.instance }}"
          description: "Docker daemon on {{ $labels.instance }} is not running for more than 30 seconds. Check service status and logs."

  - name: network_service_alerts
    rules:
      # Blackbox Monitoring
      - alert: EndpointDown
        expr: min_over_time(probe_success{job=~"blackbox_targets.*", instance=~".+"}[30s]) == 0 and up{job="blackbox_exporter"} == 1
        for: 15s
        labels:
          severity: critical
          service: network
          component: endpoint
        annotations:
          summary: "Endpoint {{ $labels.instance }} is down"
          description: "Endpoint {{ $labels.instance }} (module {{ $labels.module }}) has been unreachable for more than 15 seconds. Check endpoint availability."
      - alert: HighResponseTime
        expr: probe_duration_seconds{job=~"blackbox_targets.*", instance=~".+"} > 5
        for: 2m
        labels:
          severity: warning
          service: network
          component: latency
        annotations:
          summary: "High response time for {{ $labels.instance }}"
          description: "Response time for {{ $labels.instance }} (module {{ $labels.module }}) is above 5 seconds for more than 2 minutes. Current: {{ $value | printf \"%.2f\" }}s."
      - alert: SSLExpiryWarning
        expr: (probe_ssl_earliest_cert_expiry{job=~"blackbox_targets.*", instance=~".+"} - time()) / 86400 < 30
        for: 2m
        labels:
          severity: warning
          service: network
          component: ssl
        annotations:
          summary: "SSL certificate expiring soon for {{ $labels.instance }}"
          description: "SSL certificate for {{ $labels.instance }} (module {{ $labels.module }}) will expire in {{ $value | printf \"%.0f\" }} days. Check certificate renewal."
      - alert: SSLExpiryCritical
        expr: (probe_ssl_earliest_cert_expiry{job=~"blackbox_targets.*", instance=~".+"} - time()) / 86400 < 7
        for: 30s
        labels:
          severity: critical
          service: network
          component: ssl
        annotations:
          summary: "SSL certificate expiring very soon for {{ $labels.instance }}"
          description: "SSL certificate for {{ $labels.instance }} (module {{ $labels.module }}) will expire in {{ $value | printf \"%.0f\" }} days. Immediate renewal required."

  - name: prometheus_retention_alerts
    rules:
      # Storage Space Alerts
      - alert: PrometheusStorageSpaceHigh
        expr: (prometheus_tsdb_size_bytes{job="prometheus"} / prometheus_config_prometheus_retention_size_bytes{job="prometheus"}) * 100 > 80 and prometheus_config_prometheus_retention_size_bytes{job="prometheus"} > 0
        for: 2m
        labels:
          severity: warning
          service: prometheus-storage
        annotations:
          summary: "Prometheus storage space usage high"
          description: "Prometheus instance {{ $labels.instance }} is using {{ $value | printf \"%.2f\" }}% of configured retention size limit."
      - alert: PrometheusStorageSpaceCritical
        expr: (prometheus_tsdb_size_bytes{job="prometheus"} / prometheus_config_prometheus_retention_size_bytes{job="prometheus"}) * 100 > 95 and prometheus_config_prometheus_retention_size_bytes{job="prometheus"} > 0
        for: 1m  
        labels:
          severity: critical
          service: prometheus-storage
        annotations:
          summary: "Prometheus storage space critically high"
          description: "Prometheus instance {{ $labels.instance }} is using {{ $value | printf \"%.2f\" }}% of configured retention size limit. Immediate action required."
      # Retention Time Alerts
      - alert: PrometheusRetentionTimeHigh
        expr: prometheus_tsdb_lowest_timestamp_seconds{job="prometheus"} < (time() - (30 * 24 * 3600)) and up{job="prometheus"} == 1
        for: 5m
        labels:
          severity: warning
          service: prometheus-storage
        annotations:
          summary: "Prometheus data retention approaching limit"
          description: "Prometheus instance {{ $labels.instance }} has data older than 30 days. Retention cleanup may be needed."
      # WAL Alerts
      - alert: PrometheusWALSizeHigh
        expr: prometheus_tsdb_wal_fsync_duration_seconds{job="prometheus"} > 1 and up{job="prometheus"} == 1
        for: 2m
        labels:
          severity: warning
          service: prometheus-storage
        annotations:
          summary: "Prometheus WAL fsync duration high"
          description: "Prometheus instance {{ $labels.instance }} WAL fsync is taking {{ $value | printf \"%.2f\" }}s, indicating potential storage issues."
      # Compaction Alerts
      - alert: PrometheusCompactionFailed
        expr: rate(prometheus_tsdb_compactions_failed_total{job="prometheus"}[5m]) > 0 and up{job="prometheus"} == 1
        for: 2m
        labels:
          severity: warning
          service: prometheus-storage
        annotations:
          summary: "Prometheus compaction failures detected"
          description: "Prometheus instance {{ $labels.instance }} is experiencing compaction failures at {{ $value | printf \"%.2f\" }} failures/sec."
      # Block Loading Alerts
      - alert: PrometheusBlockLoadingFailed
        expr: rate(prometheus_tsdb_reloads_failures_total{job="prometheus"}[5m]) > 0 and up{job="prometheus"} == 1
        for: 2m
        labels:
          severity: warning
          service: prometheus-storage
        annotations:
          summary: "Prometheus block loading is failing"
          description: "Prometheus instance {{ $labels.instance }} has experienced TSDB reload failures at {{ $value | printf \"%.2f\" }} failures/sec."

  - name: loki_alerts
    rules:
      # Loki Health Check
      - alert: LokiDown
        expr: up{job="loki"} == 0
        for: 30s
        labels:
          severity: critical
          service: loki
        annotations:
          summary: "Loki is down"
          description: "Loki instance {{ $labels.instance }} has been down for more than 30 seconds. Log collection and querying is unavailable."
      # Loki Basic Health (using basic metrics available in single-binary mode)
      - alert: LokiHighMemoryUsage
        expr: process_resident_memory_bytes{job="loki"} > 1024 * 1024 * 1024
        for: 5m
        labels:
          severity: warning
          service: loki
        annotations:
          summary: "Loki high memory usage"
          description: "Loki instance {{ $labels.instance }} is using {{ $value | humanize1024 }}B of memory for more than 5 minutes."
      # Loki Request Errors (if available)
      - alert: LokiHighErrorRate
        expr: rate(loki_request_duration_seconds{status_code!~"2.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
          service: loki
        annotations:
          summary: "Loki high error rate"
          description: "Loki instance {{ $labels.instance }} is experiencing {{ $value | printf \"%.2f\" }} errors/sec for more than 2 minutes."